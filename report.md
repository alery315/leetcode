# 调研

## Park

开放的,可扩展的平台,提供RL的通用接口,有12个计算机系统环境,涵盖跨网络,数据库,分布式系统的各种问题.

Park为每个环境定义了MDP公式,例如触发MDP步骤,状态和动作空间以及奖励功能的事件,使得研究人员可以专注算法本身,而不必处理底层实现的问题.

Park可以轻松地在一个通用基准上比较不同的agent,在RL代理与后端系统之间定义了RPC接口,可以轻松地扩展到更多的环境.

## 计算机系统中的问题

### 网络

1. 拥塞控制,发送方有代理,根据先前的数据包来设置发送速率,对维持高吞吐和低延迟比较重要.

2. 视频流的比特率适配,代理基于自己的网络,去决定每个块的比特率,最大化分辨率,最小化停顿时间.

### 数据库

1. 数据库响应用户请求来有效地组织和检索数据,对索引进行索引或排列来适合检索模式很重要,代理可以观察查询模式,并据此决定如何最佳地构造,存储并随着时间的推移组织数据.

2. 查询优化,现代查询优化器是复杂的启发式方法,结合了规则,手工成本模型,数据统计和动态编程,目的是对查询运算符,进行重新排序,以最终缩短执行时间,不幸的是,现有的查询优化器不会随着时间推移来改进,也不会从错误中学习.所以他们很适合通过RL优化,这里,目标是基于优化和运行查询计划的反馈来进行查询优化策略.

### 分布式系统

 分布式系统处理的计算量很大,无法在一台计算机上容纳,用于大数据处理的spark框架通过存储在多个计算机上的数据来计算结果.代理来决定如何分配计算和内存资源来快速完成作业.

### 操作系统

操作系统在各种应用程序间有效地多路复用硬件资源(计算,内存,存储)

1. 提供一种内存层次结构,操作系统有有限数量的快速存储器和相对大量的慢速存储器,操作系统提供了高速缓存机制,可以给多个应用程序之间多路复用,驻留在高速缓存中的部分可以获得性能收益.RL代理可以观察缓存中现有对象和传入对象的信息,然后让它决定是否允许传入对象以及要从缓存中逐出哪些过时对象.目标是根据对象的访问模式来最大化缓存命中率.

2. CPU电源状态管理,操作系统控制CPU以更高的时钟速度节省能量,RL代理可以基于对每个应用程序运行方式的观察来动态控制时钟速度,(应用程序是CPU约束还是网络约束,还是IO任务的应用程序),目标是降低功耗的同时保持较高的应用程序性能.


## RL应用到计算机系统的挑战

计算机系统具有自己独特的特征与挑战,这些特性导致现成的RL方法应用到计算机系统中无法获得很好的性能.我们关注RL设计流程不同阶段中许多系统之间出现的常见挑战.

### 3.1状态-决策空间(p3)

大海捞针般难,在某些计算机系统中,大多数状态决策空间在探索的奖励反馈中没有差异.在RL训练期间,尤其是策略随机初始化的开始阶段,不会提供有意义的梯度.

1. 网络拥塞

2. 电路设计

#### 状态-决策空间的表示

在位结构复杂的问题设计RL方法时候,正确编码状态操作空间是关键挑战,在某些系统中,操作空间随着问题大小的增加而呈指数增长.

1. 交换机调度,标准的32端口,那么将有32!的匹配,对如此大的操作空间进行编码很有挑战性.

2. 作业映射到机器,映射数目以及相应的操作随着系统中新作业数目增加而增加.

为每个问题找到正确的表示形式是一个中心挑战,Spark和TensorFlow和电路设计在某种程度上是数据流图,可以利用图卷积神经网络GCN而不是LSTM可以显著提高泛化能力.

但是某些问题,比如查询优化,在很大程度上仍然没有解决.

### 3.2决策过程

马尔科夫决策过程的随机性导致了巨大的方差.由于外界的随机输入过程的影响,排队系统环境(如作业调度,负载均衡,缓存纳入策略)部分有动态性.不仅取决于系统内做出的决策,还取决于将工作带入系统的到达过程.在这些环境中,输入过程的随机性导致reward的巨大方差.

为了说明,考虑下面图一的负载均衡例子.工作序列来的不一样,导致agent获得的reward差异较大.这种差异与时间T的action无关,完全由不同的输入过程引起的.所以,用于估计动作值的标准方法存在很大差异.

先前的工作提出了一个依赖于输入的baseline,这个baseline有效地减少了输入过程的方差毛.图五显示了在负载均衡和自适应视频流环境中使用依赖于输入的baseline时的策略改进.但是实现方式(多值网络和元baseline)都是为梯度方法定制的,要求有可以重复的输入过程.对于基于值的RL方法以及输入过程无法控制的环境,应对输入驱动的方差仍然没有解决.

#### infinite horizon problems

实际上,生产计算机系统,(spark调度,负载平衡器,高速缓存控制器)长期运行并且无限期地托管服务,这样就创建了一个infinite horizon的马尔科夫决策过程,它阻止了RL agent进行 episodic (分段)训练,特别地,由于没有终端状态来容易地分配已知目标值,这给自举值带来了困难.在速度较慢的服务器上调度大型作业会阻止小型作业.这种情况下,平均RL公式可能是一个可行的选择.

### 3.3模拟与现实的差距

与模拟中训练RL不同,在实际运行的计算机系统上稳固地部署训练有素的RL agent或者直接训练RL有许多困难.首先,模拟与现实的差异阻止了直接概括.

例如,在数据库查询优化中,现有的模拟器或查询计划人员使用离线成本模型来预测查询执行时间(作为奖励的代理),然鹅,由于基础数据分布的差异和特定于系统的工件,成本模型的准确性会随着查询变得复杂而迅速下降.其次,与某些实际系统的交互可能很慢.

例如在自适应视频流中,代理控制视频每个块的比特率,因此,仅在下载视频块之后,系统才将reward返还给代理,这通常需要几秒钟.天真的使用模拟中相同的训练方法将花费单线程agent超过10年的时间才能完成现实的训练.

最后,实时训练或者直接部署模拟训练好的会降低系统表现性能.

描述负载平衡的具体示例。当使用双峰分布作业大小进行培训时，RL代理学会为小型作业保留某个服务器以快速处理它们。但是，当分布发生变化时，盲目地保留服务器会浪费计算资源并降低系统吞吐量。因此,部署在线训练算法,这些问题需要RL培养健壮的政策,确保安全(Garcıa &费尔南德斯´,2015; Achiam等，2017;Kang et al.， 2018)。

### 3.4可理解性超过现有的启发式

与ML的其他领域一样，可解释性在使学习技术切实可行方面发挥着重要作用。 但是，与基于感知的问题或博弈相比，对于系统问题，存在许多合理的良好启发式方法。 例如，计算机科学的每门入门课程都具有诸如FIFO之类的基本调度算法。 这些启发式方法通常很容易理解和调试，而学习型方法则往往不容易。 因此，使系统中的学习算法像现有的启发法一样可调试和可解释是一个关键的挑战。 在这里，一个独特的机会是构建混合解决方案，将基于学习的技术与传统启发式技术相结合。 现有的启发式方法不仅有助于引导某些问题，而且还有助于提高安全性和可推广性。 例如，如果学习的调度算法检测到输入分布明显漂移，则可以退回到简单的启发式算法。

## 4.Park平台









--------------------
